import os
from dotenv import load_dotenv
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from tqdm import tqdm


class VectorDBManager:
    def __init__(self, json_path="Datas/traveldata.json", db_path="vector_db"):
        """ ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤ ì´ˆê¸°í™” """
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            print(":warning: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            exit()
        self.json_path = os.path.join(os.getcwd(), json_path)
        self.db_path = db_path
        self.documents = []
        self.split_documents = []
        self.embedding_model = OpenAIEmbeddings(openai_api_key=self.api_key)
        self.vector_db = None

    def load_documents(self):
        """ JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ """
        if not os.path.exists(self.json_path):
            print(f":warning: JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.json_path}")
            return

        loader = JSONLoader(file_path=self.json_path, jq_schema=".records[]", text_content=False)
        self.documents = loader.load()

        if not self.documents:
            print(":warning: JSON ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì´ {len(self.documents)}ê°œ)")
            print("ğŸ” ë¡œë“œëœ ë°ì´í„° ìƒ˜í”Œ:", self.documents[:5])

        # âœ… ë°ì´í„°ê°€ Document ê°ì²´ì¸ì§€ í™•ì¸
        if isinstance(self.documents[0], Document):
            print("âœ… ë¡œë“œëœ ë°ì´í„°ëŠ” Document ê°ì²´ì…ë‹ˆë‹¤.")
        else:
            print("âš ï¸ ë¡œë“œëœ ë°ì´í„°ëŠ” Document ê°ì²´ê°€ ì•„ë‹™ë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


    def split_into_chunks(self, chunk_size=300, chunk_overlap=50):
        """ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë©”ì„œë“œ (ë©”íƒ€ë°ì´í„° ìœ ì§€) """
        if not self.documents:
            print(":warning: ë¡œë“œëœ JSON ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. `load_documents()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        self.split_documents = []
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for item in self.documents:
            # âœ… Document ê°ì²´ì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
            if isinstance(item, Document):
                metadata = item.metadata  
            elif isinstance(item, dict):  
                metadata = item
            else:
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° íƒ€ì…: {type(item)}")
                continue

            processed_metadata = {
                "id": str(metadata.get("id", "N/A")),
                "ê´€ê´‘ì§€ëª…": metadata.get("ê´€ê´‘ì§€ëª…", "N/A"),
                "ì„¤ëª…": metadata.get("ì„¤ëª…", "No description"),
                "ìœ„ì¹˜": metadata.get("ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ", metadata.get("ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ", "ì •ë³´ ì—†ìŒ"))
            }

            document = Document(page_content=f"{processed_metadata['ê´€ê´‘ì§€ëª…']}, {processed_metadata['ì„¤ëª…']}", metadata=processed_metadata)
            self.split_documents.extend(text_splitter.split_documents([document]))

        print(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ! (ì´ {len(self.split_documents)}ê°œ)")


    def generate_embeddings(self):
        """ ë¶„í• ëœ ë¬¸ì„œì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ """
        if not self.split_documents:
            print(":warning: ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `split_into_chunks()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        embeddings = self.embedding_model.embed_documents(
            [doc.page_content for doc in tqdm(self.split_documents, desc=":arrows_counterclockwise: ì„ë² ë”© ìƒì„± ì¤‘...")]
        )
        print(f"âœ…: ì„ë² ë”© ìƒì„± ì™„ë£Œ! (ì´ {len(embeddings)}ê°œ)")
        return embeddings

    def create_vector_db(self):
        """ FAISS ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ (ë©”íƒ€ë°ì´í„° ìœ ì§€) """
        if not self.split_documents:
            print(":warning: ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `split_into_chunks()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        for doc in self.split_documents:
            print(f"ğŸ“Œ ì €ì¥ ë¬¸ì„œ ë‚´ìš©: {doc.page_content}, ë©”íƒ€ë°ì´í„°: {doc.metadata}")

        self.vector_db = FAISS.from_documents(self.split_documents, self.embedding_model)
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ! (ë©”íƒ€ë°ì´í„° í¬í•¨)")

    def save_vector_db(self):
        """ ìƒì„±ëœ FAISS ë²¡í„° DBë¥¼ ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ë©”ì„œë“œ """
        if self.vector_db:
            self.vector_db.save_local(self.db_path)
            print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {self.db_path})")
        else:
            print(":warning: ì €ì¥í•  ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `create_vector_db()`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    def load_vector_db(self):
        """ ê¸°ì¡´ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ """
        if not os.path.exists(self.db_path):
            print(":warning: ë¡œë“œí•  ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `create_vector_db()`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        try:
            self.vector_db = FAISS.load_local(self.db_path, self.embedding_model, allow_dangerous_deserialization=True)
            self.faiss_index = self.vector_db.index  # ğŸ”¹ FAISS ì¸ë±ìŠ¤ë¥¼ ë³„ë„ë¡œ ì €ì¥
            print(f"âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ! (ê²½ë¡œ: {self.db_path}) | ë²¡í„° ê°œìˆ˜: {self.faiss_index.ntotal}")
        except Exception as e:
            print(f":warning: ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    def search_similar_locations(self, query, k=5):
        if not self.vector_db:
            print("âš ï¸ ë²¡í„° DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `load_vector_db()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        query_vector = self.embedding_model.embed_query(query)
        docs = self.vector_db.similarity_search_by_vector(query_vector, k=k)

        results = []
        for i, doc in enumerate(docs):
            metadata = doc.metadata if hasattr(doc, "metadata") else {}
            results.append({
                "id": metadata.get("id", str(i)),  
                "name": metadata.get("ê´€ê´‘ì§€ëª…", "ì´ë¦„ ì—†ìŒ"),
                "description": metadata.get("ì„¤ëª…", "ì„¤ëª… ì—†ìŒ"),
                "location": metadata.get("ìœ„ì¹˜", "ì •ë³´ ì—†ìŒ")  # ìœ„ì¹˜ ì¶”ê°€
            })

        return results

    def get_location_by_id(self, id):
        """ íŠ¹ì • ê´€ê´‘ì§€ IDë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ """
        if not self.vector_db:
            print("âš ï¸ ë²¡í„° DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `load_vector_db()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None

        for doc in self.split_documents:
            metadata = doc.metadata if hasattr(doc, "metadata") else {}
            if metadata.get("id") == id:
                return {
                    "name": metadata.get("ê´€ê´‘ì§€ëª…", "ì´ë¦„ ì—†ìŒ"),
                    "description": metadata.get("ì„¤ëª…", "ì„¤ëª… ì—†ìŒ"),
                    "location": metadata.get("ìœ„ì¹˜", "ì •ë³´ ì—†ìŒ")
                }

        return None