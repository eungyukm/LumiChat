import os
import json
from dotenv import load_dotenv
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from tqdm import tqdm


class VectorDBManager:
    def __init__(self, json_path="Datas/traveldata.json", db_path="vector_db"):
        """ ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤ ì´ˆê¸°í™” """
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            exit()
        self.json_path = os.path.join(os.getcwd(), json_path)
        self.db_path = db_path
        self.documents = []
        self.split_documents = []
        self.embedding_model = OpenAIEmbeddings(openai_api_key=self.api_key)
        self.vector_db = None

    def load_documents(self):
        """ JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ """
        if not os.path.exists(self.json_path):
            print(f"âŒ JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.json_path}")
            return

        with open(self.json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if "records" in data:  # ğŸ”¹ 'records' í‚¤ í™•ì¸
            self.documents = data["records"]
            print(f"âœ… JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì´ {len(self.documents)}ê°œ)")
        else:
            print("âŒ 'records' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.documents = []

        # ğŸ”¹ ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥ (ì• 5ê°œë§Œ)
        for i, doc in enumerate(self.documents[:5]):
            print(f"ğŸ“Œ [{i}] ë¬¸ì„œ: {doc}")


    def split_into_chunks(self, chunk_size=300, chunk_overlap=50):
        """ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë©”ì„œë“œ (ë©”íƒ€ë°ì´í„° ìœ ì§€) """
        if not self.documents:
            print("âŒ ë¡œë“œëœ JSON ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. `load_documents()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        self.split_documents = []
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for item in self.documents:
            # âœ… JSONì—ì„œ í•„ìš”í•œ ê°’ ì¶”ì¶œ (ì„¤ëª…ì„ 'ê´€ê´‘ì§€ì†Œê°œ'ì—ì„œ ê°€ì ¸ì˜´)
            metadata = {
                "id": str(item.get("id", "N/A")),
                "ê´€ê´‘ì§€ëª…": item.get("ê´€ê´‘ì§€ëª…", "N/A"),
                "ì„¤ëª…": item.get("ê´€ê´‘ì§€ì†Œê°œ", "No description"),  # ğŸ”¹ "ê´€ê´‘ì§€ì†Œê°œ"ë¡œ ë³€ê²½!
                "ìœ„ì¹˜": item.get("ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ", item.get("ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ", "ì •ë³´ ì—†ìŒ"))
            }

            document = Document(page_content=f"{metadata['ê´€ê´‘ì§€ëª…']}, {metadata['ì„¤ëª…']}", metadata=metadata)
            self.split_documents.extend(text_splitter.split_documents([document]))

        print(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ! (ì´ {len(self.split_documents)}ê°œ)")

    def generate_embeddings(self):
        """ ë¶„í• ëœ ë¬¸ì„œì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ """
        if not self.split_documents:
            print(":warning: ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `split_into_chunks()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        embeddings = self.embedding_model.embed_documents(
            [doc.page_content for doc in tqdm(self.split_documents, desc=":arrows_counterclockwise: ì„ë² ë”© ìƒì„± ì¤‘...")]
        )
        print(f"âœ…: ì„ë² ë”© ìƒì„± ì™„ë£Œ! (ì´ {len(embeddings)}ê°œ)")
        return embeddings

    def create_vector_db(self):
        """ FAISS ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ (ë©”íƒ€ë°ì´í„° ìœ ì§€) """
        if not self.split_documents:
            print(":warning: ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `split_into_chunks()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        for doc in self.split_documents[:1]:
            print(f"ğŸ“Œ ì €ì¥ ë¬¸ì„œ ë‚´ìš©: {doc.page_content}, ë©”íƒ€ë°ì´í„°: {doc.metadata}")

        self.vector_db = FAISS.from_documents(self.split_documents, self.embedding_model)
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ! (ë©”íƒ€ë°ì´í„° í¬í•¨)")

    def save_vector_db(self):
        """ ìƒì„±ëœ FAISS ë²¡í„° DBë¥¼ ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ë©”ì„œë“œ """
        if self.vector_db:
            self.vector_db.save_local(self.db_path)
            print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {self.db_path})")
        else:
            print(":warning: ì €ì¥í•  ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `create_vector_db()`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    def load_vector_db(self):
        """ ê¸°ì¡´ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ """
        if not os.path.exists(self.db_path):
            print(":warning: ë¡œë“œí•  ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `create_vector_db()`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        try:
            self.vector_db = FAISS.load_local(self.db_path, self.embedding_model, allow_dangerous_deserialization=True)
            self.faiss_index = self.vector_db.index  # ğŸ”¹ FAISS ì¸ë±ìŠ¤ë¥¼ ë³„ë„ë¡œ ì €ì¥
            print(f"âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ! (ê²½ë¡œ: {self.db_path}) | ë²¡í„° ê°œìˆ˜: {self.faiss_index.ntotal}")
        except Exception as e:
            print(f":warning: ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    def search_similar_locations(self, query, k=5):
        if not self.vector_db:
            print("âš ï¸ ë²¡í„° DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `load_vector_db()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        query_vector = self.embedding_model.embed_query(query)
        docs = self.vector_db.similarity_search_by_vector(query_vector, k=k)

        results = []
        for i, doc in enumerate(docs):
            metadata = doc.metadata if hasattr(doc, "metadata") else {}
            results.append({
                "id": metadata.get("id", str(i)),  
                "name": metadata.get("ê´€ê´‘ì§€ëª…", "ì´ë¦„ ì—†ìŒ"),
                "description": metadata.get("ì„¤ëª…", "ì„¤ëª… ì—†ìŒ"),
                "location": metadata.get("ìœ„ì¹˜", "ì •ë³´ ì—†ìŒ")  # ìœ„ì¹˜ ì¶”ê°€
            })

        return results

    def get_location_by_id(self, id):
        """ íŠ¹ì • ê´€ê´‘ì§€ IDë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ """
        if not self.vector_db:
            print("âš ï¸ ë²¡í„° DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `load_vector_db()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None

        for doc in self.split_documents:
            metadata = doc.metadata if hasattr(doc, "metadata") else {}
            if metadata.get("id") == id:
                return {
                    "name": metadata.get("ê´€ê´‘ì§€ëª…", "ì´ë¦„ ì—†ìŒ"),
                    "description": metadata.get("ì„¤ëª…", "ì„¤ëª… ì—†ìŒ"),
                    "location": metadata.get("ìœ„ì¹˜", "ì •ë³´ ì—†ìŒ")
                }

        return None