import os
import requests
from bs4 import BeautifulSoup
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ""

# LLM ì„¤ì • (GPT-4)
llm = ChatOpenAI(model_name="gpt-4", temperature=0.7)

# ëŒ€í™” ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë‚˜ë¬´ìœ„í‚¤ì—ì„œ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def fetch_lostark_classes():
    url = "https://namu.wiki/w/%EB%A1%9C%EC%8A%A4%ED%8A%B8%EC%95%84%ED%81%AC/%ED%81%B4%EB%9E%98%EC%8A%A4"
    headers = {"User-Agent": "Mozilla/5.0"}  # ì›¹ì‚¬ì´íŠ¸ ì°¨ë‹¨ ë°©ì§€
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")
    
    # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (ë‚˜ë¬´ìœ„í‚¤ íŠ¹ì„±ìƒ class ê¸°ë°˜ìœ¼ë¡œ ìš”ì†Œë¥¼ ì°¾ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ)
    content = soup.get_text(separator="\n", strip=True)
    
    return content

# ë²¡í„°DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def load_vector_db():
    print("ğŸ“¥ ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    wiki_data = fetch_lostark_classes()

    if not wiki_data:
        print("âš ï¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = text_splitter.create_documents([wiki_data])

    # ë²¡í„°DB ìƒì„±
    embedding = OpenAIEmbeddings()
    vector_db = Chroma.from_documents(docs, embedding)

    print("âœ… ë²¡í„°DB ìƒì„± ì™„ë£Œ!")
    return vector_db

# ë²¡í„°DB ë¡œë“œ
# vector_db = load_vector_db()

# LangChain ê²€ìƒ‰ ì²´ì¸ ì„¤ì •
if vector_db:
    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=vector_db.as_retriever(), memory=memory)
else:
    qa_chain = None

# ì±—ë´‡ ì‘ë‹µ í•¨ìˆ˜
def get_chat_response(question: str):
    if qa_chain is None:
        return "âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‚˜ë¬´ìœ„í‚¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
    
    response = qa_chain.invoke({"question": question})
    return response["answer"]

# ì±—ë´‡ ì‘ë‹µ
# response = get_chat_response("ë¡œìŠ¤íŠ¸ì•„í¬ì˜ ë²„ì„œì»¤ í´ë˜ìŠ¤ì— ì „ì‚¬ í´ë˜ìŠ¤ ì¢…ë¥˜ ì„¤ëª…í•´ì¤˜")# print(response)